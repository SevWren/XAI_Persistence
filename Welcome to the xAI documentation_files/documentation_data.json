[{"propertyName1":"#REST API\n\nThe xAI Enterprise API is a robust, high-performance RESTful interface designed for seamless integration into existing systems. It offers advanced AI capabilities with full compatibility with the OpenAI REST API.\n\n#API Key\n\n/v1/api-key\n\nGet information about an API key, including name, status, permissions and users who created or modified this key.\n\nDefinition\nExample\n\nGET\n\n/v1/api-key\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"acls\": [],\n  \"api_key_blocked\": false,\n  \"api_key_disabled\": false,\n  \"api_key_id\": \"\",\n  \"create_time\": \"\",\n  \"modified_by\": \"\",\n  \"modify_time\": \"\",\n  \"name\": \"\",\n  \"redacted_api_key\": \"\",\n  \"team_blocked\": false,\n  \"team_id\": \"\",\n  \"user_id\": \"\"\n}\n200\n400\n#Chat Completions\n\n/v1/chat/completions\n\nCreate a language model response for a given chat conversation. This endpoint is compatible with the OpenAI API.\n\nRequest body\n\nmessages\n\narray\n\nRequired\n\nA list of messages that make up the the chat conversation. Different models support different message types, such as image and text.\n\nmodel\n\nstring\n\nRequired\n\nModel name for the model to use.\n\nfrequency_penalty\n\nnumber\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\nlogit_bias\n\nobject\n\nA JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n\nlogprobs\n\nboolean\n\nWhether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.\n\nmax_tokens\n\ninteger\n\nThe maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.\n\nn\n\ninteger\n\nHow many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.\n\npresence_penalty\n\nnumber\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\nresponse_format\n\nunknown\n\nseed\n\ninteger\n\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.\n\nstop\n\narray\n\nUp to 4 sequences where the API will stop generating further tokens.\n\nstream\n\nboolean\n\nIf set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.\n\nstream_options\n\nunknown\n\ntemperature\n\nnumber\n\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\ntool_choice\n\nunknown\n\ntools\n\narray\n\nA list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\n\ntop_logprobs\n\ninteger\n\nAn integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.\n\ntop_p\n\nnumber\n\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. It is generally recommended to alter this or `temperature` but not both.\n\nuser\n\nstring\n\nA unique identifier representing your end-user, which can help xAI to monitor and detect abuse.\n\nHide optional fields\nDefinition\nExample\n\nPOST\n\n/v1/chat/completions\n\n{\n  \"frequency_penalty\": 0,\n  \"logit_bias\": {},\n  \"logprobs\": false,\n  \"max_tokens\": 0,\n  \"messages\": [],\n  \"model\": \"\",\n  \"n\": 0,\n  \"presence_penalty\": 0,\n  \"response_format\": null,\n  \"seed\": 0,\n  \"stop\": [],\n  \"stream\": false,\n  \"stream_options\": null,\n  \"temperature\": 0,\n  \"tool_choice\": null,\n  \"tools\": [],\n  \"top_logprobs\": 0,\n  \"top_p\": 0,\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"choices\": [],\n  \"created\": 0,\n  \"id\": \"\",\n  \"model\": \"\",\n  \"object\": \"\",\n  \"system_fingerprint\": \"\",\n  \"usage\": null\n}\n200\n400\n#Completions\n\n/v1/completions\n\nCreate a language model response for a given prompt. This endpoint is compatible with the OpenAI API.\n\nRequest body\n\nmodel\n\nstring\n\nRequired\n\nSpecifies the model to be used for the request.\n\nprompt\n\nunknown\n\nRequired\n\nbest_of\n\ninteger\n\nGenerates multiple completions internally and returns the top-scoring one. Not functional yet.\n\necho\n\nboolean\n\nOption to include the original prompt in the response along with the generated completion.\n\nfrequency_penalty\n\nnumber\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\nlogit_bias\n\nobject\n\nAccepts a JSON object that maps tokens to an associated bias value from -100 to 100. You can use this tokenizer tool to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n\nlogprobs\n\nboolean\n\nInclude the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to `logprobs+1` elements in the response.\n\nmax_tokens\n\ninteger\n\nLimits the number of tokens that can be produced in the output. Ensure the sum of prompt tokens and `max_tokens` does not exceed the model's context limit.\n\nn\n\ninteger\n\nDetermines how many completion sequences to produce for each prompt. Be cautious with its use due to high token consumption; adjust `max_tokens` and stop sequences accordingly.\n\npresence_penalty\n\nnumber\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\nseed\n\ninteger\n\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.\n\nstop\n\narray\n\nUp to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n\nstream\n\nboolean\n\nWhether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.\n\nstream_options\n\nunknown\n\nsuffix\n\nstring\n\nOptional string to append after the generated text.\n\ntemperature\n\nnumber\n\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.\n\ntop_p\n\nnumber\n\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both.\n\nuser\n\nstring\n\nA unique identifier representing your end-user, which can help xAI to monitor and detect abuse.\n\nHide optional fields\nDefinition\nExample\n\nPOST\n\n/v1/completions\n\n{\n  \"best_of\": 0,\n  \"echo\": false,\n  \"frequency_penalty\": 0,\n  \"logit_bias\": {},\n  \"logprobs\": false,\n  \"max_tokens\": 0,\n  \"model\": \"\",\n  \"n\": 0,\n  \"presence_penalty\": 0,\n  \"prompt\": null,\n  \"seed\": 0,\n  \"stop\": [],\n  \"stream\": false,\n  \"stream_options\": null,\n  \"suffix\": \"\",\n  \"temperature\": 0,\n  \"top_p\": 0,\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"choices\": [],\n  \"created\": 0,\n  \"id\": \"\",\n  \"model\": \"\",\n  \"object\": \"\",\n  \"system_fingerprint\": \"\",\n  \"usage\": null\n}\n200\n400\n#Create embeddings\n\n/v1/embeddings\n\nCreate an embedding vector representation corresponding to the input text. This endpoint is compatible with the OpenAI API.\n\nRequest body\n\ninput\n\nunknown\n\nRequired\n\nmodel\n\nstring\n\nRequired\n\nID of the model to use.\n\ndimensions\n\ninteger\n\nThe number of dimensions the resulting output embeddings should have.\n\nencoding_format\n\nstring\n\nThe format to return the embeddings in. Can be either `float` or `base64`.\n\nuser\n\nstring\n\nA unique identifier representing your end-user, which can help xAI to monitor and detect abuse.\n\nHide optional fields\nDefinition\nExample\n\nPOST\n\n/v1/embeddings\n\n{\n  \"dimensions\": 0,\n  \"encoding_format\": \"\",\n  \"input\": null,\n  \"model\": \"\",\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"data\": [],\n  \"model\": \"\",\n  \"object\": \"\",\n  \"usage\": null\n}\n200\n400\n#List embedding models\n\n/v1/embedding-models\n\nList all embedding models available for an API key.\n\nDefinition\nExample\n\nGET\n\n/v1/embedding-models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400\n#Get embedding model\n\n/v1/embedding-models/{model_id}\n\nEmbedding model retrieval with full information.\n\nPath parameters\n\nmodel_id\n\nstring\n\nID of the model to get.\n\nDefinition\nExample\n\nGET\n\n/v1/embedding-models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"created\": 0,\n  \"id\": \"\",\n  \"input_modalities\": [],\n  \"object\": \"\",\n  \"owned_by\": \"\",\n  \"prompt_image_token_price\": 0,\n  \"prompt_text_token_price\": 0,\n  \"version\": \"\"\n}\n200\n400\n404\n#List language models\n\n/v1/language-models\n\nList all language models available.\n\nDefinition\nExample\n\nGET\n\n/v1/language-models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400\n#Get language model\n\n/v1/language-models/{model_id}\n\nGet information about an embedding model using its ID.\n\nPath parameters\n\nmodel_id\n\nstring\n\nID of the model to get.\n\nDefinition\nExample\n\nGET\n\n/v1/language-models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"completion_text_token_price\": 0,\n  \"created\": 0,\n  \"id\": \"\",\n  \"input_modalities\": [],\n  \"object\": \"\",\n  \"output_modalities\": [],\n  \"owned_by\": \"\",\n  \"prompt_image_token_price\": 0,\n  \"prompt_text_token_price\": 0,\n  \"version\": \"\"\n}\n200\n400\n404\n#List models\n\n/v1/models\n\nOpenAI compatible version of model listing with reduced information. This endpoint is compatible with the OpenAI API.\n\nDefinition\nExample\n\nGET\n\n/v1/models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400\n#Get model\n\n/v1/models/{model_id}\n\nList all language and embedding models available. This endpoint is compatible with the OpenAI API.\n\nPath parameters\n\nmodel_id\n\nstring\n\nID of the model to get.\n\nDefinition\nExample\n\nGET\n\n/v1/models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"created\": 0,\n  \"id\": \"\",\n  \"object\": \"\",\n  \"owned_by\": \"\"\n}\n200\n400\n404","propertyName2":"Definition\nExample\n\nGET\n\n/v1/api-key\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"acls\": [],\n  \"api_key_blocked\": false,\n  \"api_key_disabled\": false,\n  \"api_key_id\": \"\",\n  \"create_time\": \"\",\n  \"modified_by\": \"\",\n  \"modify_time\": \"\",\n  \"name\": \"\",\n  \"redacted_api_key\": \"\",\n  \"team_blocked\": false,\n  \"team_id\": \"\",\n  \"user_id\": \"\"\n}\n200\n400"},{"propertyName1":"#API Key\n\n/v1/api-key\n\nGet information about an API key, including name, status, permissions and users who created or modified this key.\n\nDefinition\nExample\n\nGET\n\n/v1/api-key\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"acls\": [],\n  \"api_key_blocked\": false,\n  \"api_key_disabled\": false,\n  \"api_key_id\": \"\",\n  \"create_time\": \"\",\n  \"modified_by\": \"\",\n  \"modify_time\": \"\",\n  \"name\": \"\",\n  \"redacted_api_key\": \"\",\n  \"team_blocked\": false,\n  \"team_id\": \"\",\n  \"user_id\": \"\"\n}\n200\n400","propertyName2":"Definition\nExample\n\nPOST\n\n/v1/chat/completions\n\n{\n  \"frequency_penalty\": 0,\n  \"logit_bias\": {},\n  \"logprobs\": false,\n  \"max_tokens\": 0,\n  \"messages\": [],\n  \"model\": \"\",\n  \"n\": 0,\n  \"presence_penalty\": 0,\n  \"response_format\": null,\n  \"seed\": 0,\n  \"stop\": [],\n  \"stream\": false,\n  \"stream_options\": null,\n  \"temperature\": 0,\n  \"tool_choice\": null,\n  \"tools\": [],\n  \"top_logprobs\": 0,\n  \"top_p\": 0,\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"choices\": [],\n  \"created\": 0,\n  \"id\": \"\",\n  \"model\": \"\",\n  \"object\": \"\",\n  \"system_fingerprint\": \"\",\n  \"usage\": null\n}\n200\n400"},{"propertyName1":"#Chat Completions\n\n/v1/chat/completions\n\nCreate a language model response for a given chat conversation. This endpoint is compatible with the OpenAI API.\n\nRequest body\n\nmessages\n\narray\n\nRequired\n\nA list of messages that make up the the chat conversation. Different models support different message types, such as image and text.\n\nmodel\n\nstring\n\nRequired\n\nModel name for the model to use.\n\nfrequency_penalty\n\nnumber\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\nlogit_bias\n\nobject\n\nA JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n\nlogprobs\n\nboolean\n\nWhether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.\n\nmax_tokens\n\ninteger\n\nThe maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.\n\nn\n\ninteger\n\nHow many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.\n\npresence_penalty\n\nnumber\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\nresponse_format\n\nunknown\n\nseed\n\ninteger\n\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.\n\nstop\n\narray\n\nUp to 4 sequences where the API will stop generating further tokens.\n\nstream\n\nboolean\n\nIf set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.\n\nstream_options\n\nunknown\n\ntemperature\n\nnumber\n\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\ntool_choice\n\nunknown\n\ntools\n\narray\n\nA list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\n\ntop_logprobs\n\ninteger\n\nAn integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.\n\ntop_p\n\nnumber\n\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. It is generally recommended to alter this or `temperature` but not both.\n\nuser\n\nstring\n\nA unique identifier representing your end-user, which can help xAI to monitor and detect abuse.\n\nHide optional fields\nDefinition\nExample\n\nPOST\n\n/v1/chat/completions\n\n{\n  \"frequency_penalty\": 0,\n  \"logit_bias\": {},\n  \"logprobs\": false,\n  \"max_tokens\": 0,\n  \"messages\": [],\n  \"model\": \"\",\n  \"n\": 0,\n  \"presence_penalty\": 0,\n  \"response_format\": null,\n  \"seed\": 0,\n  \"stop\": [],\n  \"stream\": false,\n  \"stream_options\": null,\n  \"temperature\": 0,\n  \"tool_choice\": null,\n  \"tools\": [],\n  \"top_logprobs\": 0,\n  \"top_p\": 0,\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"choices\": [],\n  \"created\": 0,\n  \"id\": \"\",\n  \"model\": \"\",\n  \"object\": \"\",\n  \"system_fingerprint\": \"\",\n  \"usage\": null\n}\n200\n400","propertyName2":"Definition\nExample\n\nPOST\n\n/v1/completions\n\n{\n  \"best_of\": 0,\n  \"echo\": false,\n  \"frequency_penalty\": 0,\n  \"logit_bias\": {},\n  \"logprobs\": false,\n  \"max_tokens\": 0,\n  \"model\": \"\",\n  \"n\": 0,\n  \"presence_penalty\": 0,\n  \"prompt\": null,\n  \"seed\": 0,\n  \"stop\": [],\n  \"stream\": false,\n  \"stream_options\": null,\n  \"suffix\": \"\",\n  \"temperature\": 0,\n  \"top_p\": 0,\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"choices\": [],\n  \"created\": 0,\n  \"id\": \"\",\n  \"model\": \"\",\n  \"object\": \"\",\n  \"system_fingerprint\": \"\",\n  \"usage\": null\n}\n200\n400"},{"propertyName1":"#Completions\n\n/v1/completions\n\nCreate a language model response for a given prompt. This endpoint is compatible with the OpenAI API.\n\nRequest body\n\nmodel\n\nstring\n\nRequired\n\nSpecifies the model to be used for the request.\n\nprompt\n\nunknown\n\nRequired\n\nbest_of\n\ninteger\n\nGenerates multiple completions internally and returns the top-scoring one. Not functional yet.\n\necho\n\nboolean\n\nOption to include the original prompt in the response along with the generated completion.\n\nfrequency_penalty\n\nnumber\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\nlogit_bias\n\nobject\n\nAccepts a JSON object that maps tokens to an associated bias value from -100 to 100. You can use this tokenizer tool to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n\nlogprobs\n\nboolean\n\nInclude the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to `logprobs+1` elements in the response.\n\nmax_tokens\n\ninteger\n\nLimits the number of tokens that can be produced in the output. Ensure the sum of prompt tokens and `max_tokens` does not exceed the model's context limit.\n\nn\n\ninteger\n\nDetermines how many completion sequences to produce for each prompt. Be cautious with its use due to high token consumption; adjust `max_tokens` and stop sequences accordingly.\n\npresence_penalty\n\nnumber\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\nseed\n\ninteger\n\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.\n\nstop\n\narray\n\nUp to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n\nstream\n\nboolean\n\nWhether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.\n\nstream_options\n\nunknown\n\nsuffix\n\nstring\n\nOptional string to append after the generated text.\n\ntemperature\n\nnumber\n\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.\n\ntop_p\n\nnumber\n\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both.\n\nuser\n\nstring\n\nA unique identifier representing your end-user, which can help xAI to monitor and detect abuse.\n\nHide optional fields\nDefinition\nExample\n\nPOST\n\n/v1/completions\n\n{\n  \"best_of\": 0,\n  \"echo\": false,\n  \"frequency_penalty\": 0,\n  \"logit_bias\": {},\n  \"logprobs\": false,\n  \"max_tokens\": 0,\n  \"model\": \"\",\n  \"n\": 0,\n  \"presence_penalty\": 0,\n  \"prompt\": null,\n  \"seed\": 0,\n  \"stop\": [],\n  \"stream\": false,\n  \"stream_options\": null,\n  \"suffix\": \"\",\n  \"temperature\": 0,\n  \"top_p\": 0,\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"choices\": [],\n  \"created\": 0,\n  \"id\": \"\",\n  \"model\": \"\",\n  \"object\": \"\",\n  \"system_fingerprint\": \"\",\n  \"usage\": null\n}\n200\n400","propertyName2":"Definition\nExample\n\nPOST\n\n/v1/embeddings\n\n{\n  \"dimensions\": 0,\n  \"encoding_format\": \"\",\n  \"input\": null,\n  \"model\": \"\",\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"data\": [],\n  \"model\": \"\",\n  \"object\": \"\",\n  \"usage\": null\n}\n200\n400"},{"propertyName1":"#Create embeddings\n\n/v1/embeddings\n\nCreate an embedding vector representation corresponding to the input text. This endpoint is compatible with the OpenAI API.\n\nRequest body\n\ninput\n\nunknown\n\nRequired\n\nmodel\n\nstring\n\nRequired\n\nID of the model to use.\n\ndimensions\n\ninteger\n\nThe number of dimensions the resulting output embeddings should have.\n\nencoding_format\n\nstring\n\nThe format to return the embeddings in. Can be either `float` or `base64`.\n\nuser\n\nstring\n\nA unique identifier representing your end-user, which can help xAI to monitor and detect abuse.\n\nHide optional fields\nDefinition\nExample\n\nPOST\n\n/v1/embeddings\n\n{\n  \"dimensions\": 0,\n  \"encoding_format\": \"\",\n  \"input\": null,\n  \"model\": \"\",\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"data\": [],\n  \"model\": \"\",\n  \"object\": \"\",\n  \"usage\": null\n}\n200\n400","propertyName2":"Definition\nExample\n\nGET\n\n/v1/embedding-models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400"},{"propertyName1":"#List embedding models\n\n/v1/embedding-models\n\nList all embedding models available for an API key.\n\nDefinition\nExample\n\nGET\n\n/v1/embedding-models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400","propertyName2":"Definition\nExample\n\nGET\n\n/v1/embedding-models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"created\": 0,\n  \"id\": \"\",\n  \"input_modalities\": [],\n  \"object\": \"\",\n  \"owned_by\": \"\",\n  \"prompt_image_token_price\": 0,\n  \"prompt_text_token_price\": 0,\n  \"version\": \"\"\n}\n200\n400\n404"},{"propertyName1":"#Get embedding model\n\n/v1/embedding-models/{model_id}\n\nEmbedding model retrieval with full information.\n\nPath parameters\n\nmodel_id\n\nstring\n\nID of the model to get.\n\nDefinition\nExample\n\nGET\n\n/v1/embedding-models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"created\": 0,\n  \"id\": \"\",\n  \"input_modalities\": [],\n  \"object\": \"\",\n  \"owned_by\": \"\",\n  \"prompt_image_token_price\": 0,\n  \"prompt_text_token_price\": 0,\n  \"version\": \"\"\n}\n200\n400\n404","propertyName2":"Definition\nExample\n\nGET\n\n/v1/language-models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400"},{"propertyName1":"#List language models\n\n/v1/language-models\n\nList all language models available.\n\nDefinition\nExample\n\nGET\n\n/v1/language-models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400","propertyName2":"Definition\nExample\n\nGET\n\n/v1/language-models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"completion_text_token_price\": 0,\n  \"created\": 0,\n  \"id\": \"\",\n  \"input_modalities\": [],\n  \"object\": \"\",\n  \"output_modalities\": [],\n  \"owned_by\": \"\",\n  \"prompt_image_token_price\": 0,\n  \"prompt_text_token_price\": 0,\n  \"version\": \"\"\n}\n200\n400\n404"},{"propertyName1":"#Get language model\n\n/v1/language-models/{model_id}\n\nGet information about an embedding model using its ID.\n\nPath parameters\n\nmodel_id\n\nstring\n\nID of the model to get.\n\nDefinition\nExample\n\nGET\n\n/v1/language-models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"completion_text_token_price\": 0,\n  \"created\": 0,\n  \"id\": \"\",\n  \"input_modalities\": [],\n  \"object\": \"\",\n  \"output_modalities\": [],\n  \"owned_by\": \"\",\n  \"prompt_image_token_price\": 0,\n  \"prompt_text_token_price\": 0,\n  \"version\": \"\"\n}\n200\n400\n404","propertyName2":"Definition\nExample\n\nGET\n\n/v1/models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400"},{"propertyName1":"#List models\n\n/v1/models\n\nOpenAI compatible version of model listing with reduced information. This endpoint is compatible with the OpenAI API.\n\nDefinition\nExample\n\nGET\n\n/v1/models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400","propertyName2":"Definition\nExample\n\nGET\n\n/v1/models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"created\": 0,\n  \"id\": \"\",\n  \"object\": \"\",\n  \"owned_by\": \"\"\n}\n200\n400\n404"},{"propertyName1":"#Get model\n\n/v1/models/{model_id}\n\nList all language and embedding models available. This endpoint is compatible with the OpenAI API.\n\nPath parameters\n\nmodel_id\n\nstring\n\nID of the model to get.\n\nDefinition\nExample\n\nGET\n\n/v1/models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"created\": 0,\n  \"id\": \"\",\n  \"object\": \"\",\n  \"owned_by\": \"\"\n}\n200\n400\n404","propertyName2":null}]