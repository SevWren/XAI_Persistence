[{"propertyName1":"#REST API\n\nThe xAI Enterprise API is a robust, high-performance RESTful interface designed for seamless integration into existing systems. It offers advanced AI capabilities with full compatibility with the OpenAI REST API.\n\n#API Key\n\n/v1/api-key\n\nGet information about an API key, including name, status, permissions and users who created or modified this key.\n\nDefinition\nExample\n\nGET\n\n/v1/api-key\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"acls\": [],\n  \"api_key_blocked\": false,\n  \"api_key_disabled\": false,\n  \"api_key_id\": \"\",\n  \"create_time\": \"\",\n  \"modified_by\": \"\",\n  \"modify_time\": \"\",\n  \"name\": \"\",\n  \"redacted_api_key\": \"\",\n  \"team_blocked\": false,\n  \"team_id\": \"\",\n  \"user_id\": \"\"\n}\n200\n400\n#Chat Completions\n\n/v1/chat/completions\n\nCreate a language model response for a given chat conversation. This endpoint is compatible with the OpenAI API.\n\nRequest body\n\nmessages\n\narray\n\nRequired\n\nA list of messages that make up the the chat conversation. Different models support different message types, such as image and text.\n\nmodel\n\nstring\n\nRequired\n\nModel name for the model to use.\n\nfrequency_penalty\n\nnumber\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\nlogit_bias\n\nobject\n\nA JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n\nlogprobs\n\nboolean\n\nWhether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.\n\nmax_tokens\n\ninteger\n\nThe maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.\n\nn\n\ninteger\n\nHow many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.\n\npresence_penalty\n\nnumber\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\nresponse_format\n\nunknown\n\nseed\n\ninteger\n\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.\n\nstop\n\narray\n\nUp to 4 sequences where the API will stop generating further tokens.\n\nstream\n\nboolean\n\nIf set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.\n\nstream_options\n\nunknown\n\ntemperature\n\nnumber\n\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\ntool_choice\n\nunknown\n\ntools\n\narray\n\nA list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\n\ntop_logprobs\n\ninteger\n\nAn integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.\n\ntop_p\n\nnumber\n\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. It is generally recommended to alter this or `temperature` but not both.\n\nuser\n\nstring\n\nA unique identifier representing your end-user, which can help xAI to monitor and detect abuse.\n\nHide optional fields\nDefinition\nExample\n\nPOST\n\n/v1/chat/completions\n\n{\n  \"frequency_penalty\": 0,\n  \"logit_bias\": {},\n  \"logprobs\": false,\n  \"max_tokens\": 0,\n  \"messages\": [],\n  \"model\": \"\",\n  \"n\": 0,\n  \"presence_penalty\": 0,\n  \"response_format\": null,\n  \"seed\": 0,\n  \"stop\": [],\n  \"stream\": false,\n  \"stream_options\": null,\n  \"temperature\": 0,\n  \"tool_choice\": null,\n  \"tools\": [],\n  \"top_logprobs\": 0,\n  \"top_p\": 0,\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"choices\": [],\n  \"created\": 0,\n  \"id\": \"\",\n  \"model\": \"\",\n  \"object\": \"\",\n  \"system_fingerprint\": \"\",\n  \"usage\": null\n}\n200\n400\n#Completions\n\n/v1/completions\n\nCreate a language model response for a given prompt. This endpoint is compatible with the OpenAI API.\n\nRequest body\n\nmodel\n\nstring\n\nRequired\n\nSpecifies the model to be used for the request.\n\nprompt\n\nunknown\n\nRequired\n\nbest_of\n\ninteger\n\nGenerates multiple completions internally and returns the top-scoring one. Not functional yet.\n\necho\n\nboolean\n\nOption to include the original prompt in the response along with the generated completion.\n\nfrequency_penalty\n\nnumber\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\nlogit_bias\n\nobject\n\nAccepts a JSON object that maps tokens to an associated bias value from -100 to 100. You can use this tokenizer tool to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n\nlogprobs\n\nboolean\n\nInclude the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to `logprobs+1` elements in the response.\n\nmax_tokens\n\ninteger\n\nLimits the number of tokens that can be produced in the output. Ensure the sum of prompt tokens and `max_tokens` does not exceed the model's context limit.\n\nn\n\ninteger\n\nDetermines how many completion sequences to produce for each prompt. Be cautious with its use due to high token consumption; adjust `max_tokens` and stop sequences accordingly.\n\npresence_penalty\n\nnumber\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\nseed\n\ninteger\n\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.\n\nstop\n\narray\n\nUp to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n\nstream\n\nboolean\n\nWhether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.\n\nstream_options\n\nunknown\n\nsuffix\n\nstring\n\nOptional string to append after the generated text.\n\ntemperature\n\nnumber\n\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.\n\ntop_p\n\nnumber\n\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both.\n\nuser\n\nstring\n\nA unique identifier representing your end-user, which can help xAI to monitor and detect abuse.\n\nHide optional fields\nDefinition\nExample\n\nPOST\n\n/v1/completions\n\n{\n  \"best_of\": 0,\n  \"echo\": false,\n  \"frequency_penalty\": 0,\n  \"logit_bias\": {},\n  \"logprobs\": false,\n  \"max_tokens\": 0,\n  \"model\": \"\",\n  \"n\": 0,\n  \"presence_penalty\": 0,\n  \"prompt\": null,\n  \"seed\": 0,\n  \"stop\": [],\n  \"stream\": false,\n  \"stream_options\": null,\n  \"suffix\": \"\",\n  \"temperature\": 0,\n  \"top_p\": 0,\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"choices\": [],\n  \"created\": 0,\n  \"id\": \"\",\n  \"model\": \"\",\n  \"object\": \"\",\n  \"system_fingerprint\": \"\",\n  \"usage\": null\n}\n200\n400\n#Create embeddings\n\n/v1/embeddings\n\nCreate an embedding vector representation corresponding to the input text. This endpoint is compatible with the OpenAI API.\n\nRequest body\n\ninput\n\nunknown\n\nRequired\n\nmodel\n\nstring\n\nRequired\n\nID of the model to use.\n\ndimensions\n\ninteger\n\nThe number of dimensions the resulting output embeddings should have.\n\nencoding_format\n\nstring\n\nThe format to return the embeddings in. Can be either `float` or `base64`.\n\nuser\n\nstring\n\nA unique identifier representing your end-user, which can help xAI to monitor and detect abuse.\n\nHide optional fields\nDefinition\nExample\n\nPOST\n\n/v1/embeddings\n\n{\n  \"dimensions\": 0,\n  \"encoding_format\": \"\",\n  \"input\": null,\n  \"model\": \"\",\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"data\": [],\n  \"model\": \"\",\n  \"object\": \"\",\n  \"usage\": null\n}\n200\n400\n#List embedding models\n\n/v1/embedding-models\n\nList all embedding models available for an API key.\n\nDefinition\nExample\n\nGET\n\n/v1/embedding-models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400\n#Get embedding model\n\n/v1/embedding-models/{model_id}\n\nEmbedding model retrieval with full information.\n\nPath parameters\n\nmodel_id\n\nstring\n\nID of the model to get.\n\nDefinition\nExample\n\nGET\n\n/v1/embedding-models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"created\": 0,\n  \"id\": \"\",\n  \"input_modalities\": [],\n  \"object\": \"\",\n  \"owned_by\": \"\",\n  \"prompt_image_token_price\": 0,\n  \"prompt_text_token_price\": 0,\n  \"version\": \"\"\n}\n200\n400\n404\n#List language models\n\n/v1/language-models\n\nList all language models available.\n\nDefinition\nExample\n\nGET\n\n/v1/language-models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400\n#Get language model\n\n/v1/language-models/{model_id}\n\nGet information about an embedding model using its ID.\n\nPath parameters\n\nmodel_id\n\nstring\n\nID of the model to get.\n\nDefinition\nExample\n\nGET\n\n/v1/language-models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"completion_text_token_price\": 0,\n  \"created\": 0,\n  \"id\": \"\",\n  \"input_modalities\": [],\n  \"object\": \"\",\n  \"output_modalities\": [],\n  \"owned_by\": \"\",\n  \"prompt_image_token_price\": 0,\n  \"prompt_text_token_price\": 0,\n  \"version\": \"\"\n}\n200\n400\n404\n#List models\n\n/v1/models\n\nOpenAI compatible version of model listing with reduced information. This endpoint is compatible with the OpenAI API.\n\nDefinition\nExample\n\nGET\n\n/v1/models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400\n#Get model\n\n/v1/models/{model_id}\n\nList all language and embedding models available. This endpoint is compatible with the OpenAI API.\n\nPath parameters\n\nmodel_id\n\nstring\n\nID of the model to get.\n\nDefinition\nExample\n\nGET\n\n/v1/models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"created\": 0,\n  \"id\": \"\",\n  \"object\": \"\",\n  \"owned_by\": \"\"\n}\n200\n400\n404","propertyName2":"Definition\nExample\n\nGET\n\n/v1/api-key\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"acls\": [],\n  \"api_key_blocked\": false,\n  \"api_key_disabled\": false,\n  \"api_key_id\": \"\",\n  \"create_time\": \"\",\n  \"modified_by\": \"\",\n  \"modify_time\": \"\",\n  \"name\": \"\",\n  \"redacted_api_key\": \"\",\n  \"team_blocked\": false,\n  \"team_id\": \"\",\n  \"user_id\": \"\"\n}\n200\n400","propertyName3":"#Integrations\n\nThis page contains information about how to integrate the xAI APIs into your application using REST, gRPC or our Python SDK.\n\nOur API is also fully compatible with OpenAI and Anthropic SDKs for easy migration.\n\n#OpenAI SDK\n\nThe xAI API offers compatibility with the OpenAI SDKs to support developers and their apps with minimal changes. Once you update the base URL, you can start using the SDKs to make calls to your favorite Grok models with your xAI API key.\n\n#JavaScript\n\nYou can import the OpenAI client from \nopenai\n into your Javascript application and change the base URL and API key.\n\njavascript\n\nimport OpenAI from \"openai\";\nconst openai = new OpenAI({\n  apiKey: \"<api key>\",\n  baseURL: \"https://api.x.ai/v1\",\n});\n\nconst completion = await openai.chat.completions.create({\n  model: \"grok-beta\",\n  messages: [\n    { role: \"system\", content: \"You are Grok, a chatbot inspired by the Hitchhiker's Guide to the Galaxy.\" },\n    {\n      role: \"user\",\n      content: \"What is the meaning of life, the universe, and everything?\",\n    },\n  ],\n});\n\nconsole.log(completion.choices[0].message);\n\n#Python\n\nYou can use the \nopenai\n library to interact with the Grok API in your python program.\n\npython\n\nimport os\nfrom openai import OpenAI\n\nXAI_API_KEY = os.getenv(\"XAI_API_KEY\")\nclient = OpenAI(\n    api_key=XAI_API_KEY,\n    base_url=\"https://api.x.ai/v1\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"grok-beta\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are Grok, a chatbot inspired by the Hitchhikers Guide to the Galaxy.\"},\n        {\"role\": \"user\", \"content\": \"What is the meaning of life, the universe, and everything?\"},\n    ],\n)\n\nprint(completion.choices[0].message)\n\n#Anthropic SDK\n\nxAI SDKs are also fully compatible with the Anthropic SDKs. This allows developers to easily integrate xAI's models into their existing applications. You just need to update the base URL, API key and model name. Below, you can find examples of how to use your xAI API Keys with the Anthropic SDKs.\n\n#JavaScript\n\nYou can import the Anthropic SDK from \n@anthropic-ai/sdk\n and use it to create a client instance with your xAI API Key.\n\njavascript\n\nimport Anthropic from \"@anthropic-ai/sdk\";\n\nconst anthropic = new Anthropic({\n  apiKey: \"<api key>\",\n  baseURL: \"https://api.x.ai/\",\n});\n\nconst msg = await anthropic.messages.create({\n  model: \"grok-beta\",\n  max_tokens: 128,\n  system:\n    \"You are Grok, a chatbot inspired by the Hitchhiker's Guide to the Galaxy.\",\n  messages: [\n    {\n      role: \"user\",\n      content: \"What is the meaning of life, the universe, and everything?\",\n    },\n  ],\n});\n\nconsole.log(msg);\n\n#Python\n\nLikewise, in Python, you can use the \nAnthropic\n class to create a client and send a message to the Grok model:\n\npython\n\nimport os\nfrom anthropic import Anthropic\n\nXAI_API_KEY = os.getenv(\"XAI_API_KEY\")\nclient = Anthropic(\n    api_key=XAI_API_KEY,\n    base_url=\"https://api.x.ai\",\n)\nmessage = client.messages.create(\n    model=\"grok-beta\",\n    max_tokens=128,\n    system=\"You are Grok, a chatbot inspired by the Hitchhiker's Guide to the Galaxy.\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the meaning of life, the universe, and everything?\",\n        },\n    ],\n)\nprint(message.content)","propertyName4":"#Integrations\n\nThis page contains information about how to integrate the xAI APIs into your application using REST, gRPC or our Python SDK.\n\nOur API is also fully compatible with OpenAI and Anthropic SDKs for easy migration.\n\n#OpenAI SDK\n\nThe xAI API offers compatibility with the OpenAI SDKs to support developers and their apps with minimal changes. Once you update the base URL, you can start using the SDKs to make calls to your favorite Grok models with your xAI API key.\n\n#JavaScript\n\nYou can import the OpenAI client from \nopenai\n into your Javascript application and change the base URL and API key.\n\njavascript\n\nimport OpenAI from \"openai\";\nconst openai = new OpenAI({\n  apiKey: \"<api key>\",\n  baseURL: \"https://api.x.ai/v1\",\n});\n\nconst completion = await openai.chat.completions.create({\n  model: \"grok-beta\",\n  messages: [\n    { role: \"system\", content: \"You are Grok, a chatbot inspired by the Hitchhiker's Guide to the Galaxy.\" },\n    {\n      role: \"user\",\n      content: \"What is the meaning of life, the universe, and everything?\",\n    },\n  ],\n});\n\nconsole.log(completion.choices[0].message);\n\n#Python\n\nYou can use the \nopenai\n library to interact with the Grok API in your python program.\n\npython\n\nimport os\nfrom openai import OpenAI\n\nXAI_API_KEY = os.getenv(\"XAI_API_KEY\")\nclient = OpenAI(\n    api_key=XAI_API_KEY,\n    base_url=\"https://api.x.ai/v1\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"grok-beta\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are Grok, a chatbot inspired by the Hitchhikers Guide to the Galaxy.\"},\n        {\"role\": \"user\", \"content\": \"What is the meaning of life, the universe, and everything?\"},\n    ],\n)\n\nprint(completion.choices[0].message)\n\n#Anthropic SDK\n\nxAI SDKs are also fully compatible with the Anthropic SDKs. This allows developers to easily integrate xAI's models into their existing applications. You just need to update the base URL, API key and model name. Below, you can find examples of how to use your xAI API Keys with the Anthropic SDKs.\n\n#JavaScript\n\nYou can import the Anthropic SDK from \n@anthropic-ai/sdk\n and use it to create a client instance with your xAI API Key.\n\njavascript\n\nimport Anthropic from \"@anthropic-ai/sdk\";\n\nconst anthropic = new Anthropic({\n  apiKey: \"<api key>\",\n  baseURL: \"https://api.x.ai/\",\n});\n\nconst msg = await anthropic.messages.create({\n  model: \"grok-beta\",\n  max_tokens: 128,\n  system:\n    \"You are Grok, a chatbot inspired by the Hitchhiker's Guide to the Galaxy.\",\n  messages: [\n    {\n      role: \"user\",\n      content: \"What is the meaning of life, the universe, and everything?\",\n    },\n  ],\n});\n\nconsole.log(msg);\n\n#Python\n\nLikewise, in Python, you can use the \nAnthropic\n class to create a client and send a message to the Grok model:\n\npython\n\nimport os\nfrom anthropic import Anthropic\n\nXAI_API_KEY = os.getenv(\"XAI_API_KEY\")\nclient = Anthropic(\n    api_key=XAI_API_KEY,\n    base_url=\"https://api.x.ai\",\n)\nmessage = client.messages.create(\n    model=\"grok-beta\",\n    max_tokens=128,\n    system=\"You are Grok, a chatbot inspired by the Hitchhiker's Guide to the Galaxy.\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the meaning of life, the universe, and everything?\",\n        },\n    ],\n)\nprint(message.content)","propertyName5":"#Overview\n\nThis page provides an overview of the xAI API, including authentication and example requests.\n\n#Introduction\n\nOnce you have generated an API key following the instructions in our quickstart page, you are now ready to start making requests.\n\nInteract with xAI's API using HTTP requests from any language or via Python SDKs as the API is designed for compatibility with both OpenAI's and Anthropic's SDK frameworks for a seamless transition.\n\n#Authentication\n\nTo interact with xAI's API, you'll need to authenticate your requests. There are steps to follow:\n\nFirst, you will need to visit the xAI Console to create an API key. Later, you will need to include the API key in the \nAuthorization\n header of your requests. For more information, you can refer to our Quickstart guide.\n\nbash\n\nAuthorization: Bearer YOUR_XAI_API_KEY\n\n#Making requests\n\nbash\n\ncurl https://api.x.ai/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $XAI_API_KEY\" \\\n  -d '{\n      \"messages\": [\n        {\n          \"role\": \"system\",\n          \"content\": \"You are Grok, a chatbot inspired by the Hitchhikers Guide to the Galaxy.\"\n        },\n        {\n          \"role\": \"user\",\n          \"content\": \"What is the answer to life and universe?\"\n        }\n      ],\n      \"model\": \"grok-beta\",\n      \"stream\": false,\n      \"temperature\": 0\n    }'\n\n\nThis request queries the \ngrok-beta\n model and returns a response, which will resemble the following:\n\njson\n\n{\n  \"id\": \"304e12ef-81f4-4e93-a41c-f5f57f6a2b56\",\n  \"object\": \"chat.completion\",\n  \"created\": 1728511727,\n  \"model\": \"grok-beta\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"The answer to the ultimate question of life, the universe, and everything is **42**, according to Douglas Adams science fiction series \\\"The Hitchhiker's Guide to the Galaxy.\\\" This number is often humorously referenced in discussions about the meaning of life. However, in the context of the story, the actual question to which 42 is the answer remains unknown, symbolizing the ongoing search for understanding the purpose or meaning of existence.\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 24,\n    \"completion_tokens\": 91,\n    \"total_tokens\": 115\n  },\n  \"system_fingerprint\": \"fp_3813298403\"\n}\n\n\nIn the response above, there are a few fields that are worth noting. As you can see from the \nfinish_reason\n, the model has finished generating the response without running into any limits. The \nusage\n field provides information about the number of tokens used in the request and response, which can be useful for monitoring your usage after each request. Finally, the \nsystem_fingerprint\n represent the unique configuration of the model and the backend. This fingerprint changes when any modifications are made in one of those two.\n\nYou can view the full response schema for chat completions endpoint here.\n\n#Streaming\n\nThe xAI APIs support streaming responses. This means that the response is sent to the client in chunks, rather than all at once. This is useful for applications that need to process the response as it is received, such as chatbots or text editors.\n\nTo achieve this, Server-Sent Events (SSE) standards are used. Streaming is available for chat completion endpoint here and message endpoint here (following Anthropic standards in this case). For example a streaming request looks like:\n\nbash\n\ncurl https://api.x.ai/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $XAI_API_KEY\" \\\n  -d '{\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are Grok, a chatbot inspired by the Hitchhikers Guide to the Galaxy.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"What is the answer to life and universe?\"\n      }\n    ],\n    \"model\": \"grok-beta\",\n    \"stream\": true,\n    \"temperature\": 0\n  }'\n\n\nA list of event would then be returned with \ndata\n looking like:\n\njson\n\n{\n  \"id\": \"304e12ef-81f4-4e93-a41c-f5f57f6a2b56\",\n  \"object\": \"chat.completion\",\n  \"created\": 1728511727,\n  \"model\": \"grok-beta\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"The \"\n      },\n      \"finish_reason\": \"\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 24,\n    \"completion_tokens\": 1,\n    \"total_tokens\": 25\n  },\n  \"system_fingerprint\": \"fp_3813298403\"\n}\n\n\nFollowing the last message, the backend will send a \n[DONE]\n message to close the stream.\n\nOur API's streaming capabilities are natively compatible with OpenAI and Anthropic SDKs.","propertyName6":null},{"propertyName1":"#API Key\n\n/v1/api-key\n\nGet information about an API key, including name, status, permissions and users who created or modified this key.\n\nDefinition\nExample\n\nGET\n\n/v1/api-key\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"acls\": [],\n  \"api_key_blocked\": false,\n  \"api_key_disabled\": false,\n  \"api_key_id\": \"\",\n  \"create_time\": \"\",\n  \"modified_by\": \"\",\n  \"modify_time\": \"\",\n  \"name\": \"\",\n  \"redacted_api_key\": \"\",\n  \"team_blocked\": false,\n  \"team_id\": \"\",\n  \"user_id\": \"\"\n}\n200\n400","propertyName2":"Definition\nExample\n\nPOST\n\n/v1/chat/completions\n\n{\n  \"frequency_penalty\": 0,\n  \"logit_bias\": {},\n  \"logprobs\": false,\n  \"max_tokens\": 0,\n  \"messages\": [],\n  \"model\": \"\",\n  \"n\": 0,\n  \"presence_penalty\": 0,\n  \"response_format\": null,\n  \"seed\": 0,\n  \"stop\": [],\n  \"stream\": false,\n  \"stream_options\": null,\n  \"temperature\": 0,\n  \"tool_choice\": null,\n  \"tools\": [],\n  \"top_logprobs\": 0,\n  \"top_p\": 0,\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"choices\": [],\n  \"created\": 0,\n  \"id\": \"\",\n  \"model\": \"\",\n  \"object\": \"\",\n  \"system_fingerprint\": \"\",\n  \"usage\": null\n}\n200\n400","propertyName3":null,"propertyName4":null,"propertyName5":null,"propertyName6":null},{"propertyName1":"#Chat Completions\n\n/v1/chat/completions\n\nCreate a language model response for a given chat conversation. This endpoint is compatible with the OpenAI API.\n\nRequest body\n\nmessages\n\narray\n\nRequired\n\nA list of messages that make up the the chat conversation. Different models support different message types, such as image and text.\n\nmodel\n\nstring\n\nRequired\n\nModel name for the model to use.\n\nfrequency_penalty\n\nnumber\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\nlogit_bias\n\nobject\n\nA JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n\nlogprobs\n\nboolean\n\nWhether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.\n\nmax_tokens\n\ninteger\n\nThe maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.\n\nn\n\ninteger\n\nHow many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.\n\npresence_penalty\n\nnumber\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\nresponse_format\n\nunknown\n\nseed\n\ninteger\n\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.\n\nstop\n\narray\n\nUp to 4 sequences where the API will stop generating further tokens.\n\nstream\n\nboolean\n\nIf set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.\n\nstream_options\n\nunknown\n\ntemperature\n\nnumber\n\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\ntool_choice\n\nunknown\n\ntools\n\narray\n\nA list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\n\ntop_logprobs\n\ninteger\n\nAn integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.\n\ntop_p\n\nnumber\n\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. It is generally recommended to alter this or `temperature` but not both.\n\nuser\n\nstring\n\nA unique identifier representing your end-user, which can help xAI to monitor and detect abuse.\n\nHide optional fields\nDefinition\nExample\n\nPOST\n\n/v1/chat/completions\n\n{\n  \"frequency_penalty\": 0,\n  \"logit_bias\": {},\n  \"logprobs\": false,\n  \"max_tokens\": 0,\n  \"messages\": [],\n  \"model\": \"\",\n  \"n\": 0,\n  \"presence_penalty\": 0,\n  \"response_format\": null,\n  \"seed\": 0,\n  \"stop\": [],\n  \"stream\": false,\n  \"stream_options\": null,\n  \"temperature\": 0,\n  \"tool_choice\": null,\n  \"tools\": [],\n  \"top_logprobs\": 0,\n  \"top_p\": 0,\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"choices\": [],\n  \"created\": 0,\n  \"id\": \"\",\n  \"model\": \"\",\n  \"object\": \"\",\n  \"system_fingerprint\": \"\",\n  \"usage\": null\n}\n200\n400","propertyName2":"Definition\nExample\n\nPOST\n\n/v1/completions\n\n{\n  \"best_of\": 0,\n  \"echo\": false,\n  \"frequency_penalty\": 0,\n  \"logit_bias\": {},\n  \"logprobs\": false,\n  \"max_tokens\": 0,\n  \"model\": \"\",\n  \"n\": 0,\n  \"presence_penalty\": 0,\n  \"prompt\": null,\n  \"seed\": 0,\n  \"stop\": [],\n  \"stream\": false,\n  \"stream_options\": null,\n  \"suffix\": \"\",\n  \"temperature\": 0,\n  \"top_p\": 0,\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"choices\": [],\n  \"created\": 0,\n  \"id\": \"\",\n  \"model\": \"\",\n  \"object\": \"\",\n  \"system_fingerprint\": \"\",\n  \"usage\": null\n}\n200\n400","propertyName3":null,"propertyName4":null,"propertyName5":null,"propertyName6":null},{"propertyName1":"#Completions\n\n/v1/completions\n\nCreate a language model response for a given prompt. This endpoint is compatible with the OpenAI API.\n\nRequest body\n\nmodel\n\nstring\n\nRequired\n\nSpecifies the model to be used for the request.\n\nprompt\n\nunknown\n\nRequired\n\nbest_of\n\ninteger\n\nGenerates multiple completions internally and returns the top-scoring one. Not functional yet.\n\necho\n\nboolean\n\nOption to include the original prompt in the response along with the generated completion.\n\nfrequency_penalty\n\nnumber\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\nlogit_bias\n\nobject\n\nAccepts a JSON object that maps tokens to an associated bias value from -100 to 100. You can use this tokenizer tool to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n\nlogprobs\n\nboolean\n\nInclude the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to `logprobs+1` elements in the response.\n\nmax_tokens\n\ninteger\n\nLimits the number of tokens that can be produced in the output. Ensure the sum of prompt tokens and `max_tokens` does not exceed the model's context limit.\n\nn\n\ninteger\n\nDetermines how many completion sequences to produce for each prompt. Be cautious with its use due to high token consumption; adjust `max_tokens` and stop sequences accordingly.\n\npresence_penalty\n\nnumber\n\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\nseed\n\ninteger\n\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.\n\nstop\n\narray\n\nUp to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n\nstream\n\nboolean\n\nWhether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.\n\nstream_options\n\nunknown\n\nsuffix\n\nstring\n\nOptional string to append after the generated text.\n\ntemperature\n\nnumber\n\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.\n\ntop_p\n\nnumber\n\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both.\n\nuser\n\nstring\n\nA unique identifier representing your end-user, which can help xAI to monitor and detect abuse.\n\nHide optional fields\nDefinition\nExample\n\nPOST\n\n/v1/completions\n\n{\n  \"best_of\": 0,\n  \"echo\": false,\n  \"frequency_penalty\": 0,\n  \"logit_bias\": {},\n  \"logprobs\": false,\n  \"max_tokens\": 0,\n  \"model\": \"\",\n  \"n\": 0,\n  \"presence_penalty\": 0,\n  \"prompt\": null,\n  \"seed\": 0,\n  \"stop\": [],\n  \"stream\": false,\n  \"stream_options\": null,\n  \"suffix\": \"\",\n  \"temperature\": 0,\n  \"top_p\": 0,\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"choices\": [],\n  \"created\": 0,\n  \"id\": \"\",\n  \"model\": \"\",\n  \"object\": \"\",\n  \"system_fingerprint\": \"\",\n  \"usage\": null\n}\n200\n400","propertyName2":"Definition\nExample\n\nPOST\n\n/v1/embeddings\n\n{\n  \"dimensions\": 0,\n  \"encoding_format\": \"\",\n  \"input\": null,\n  \"model\": \"\",\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"data\": [],\n  \"model\": \"\",\n  \"object\": \"\",\n  \"usage\": null\n}\n200\n400","propertyName3":null,"propertyName4":null,"propertyName5":null,"propertyName6":null},{"propertyName1":"#Create embeddings\n\n/v1/embeddings\n\nCreate an embedding vector representation corresponding to the input text. This endpoint is compatible with the OpenAI API.\n\nRequest body\n\ninput\n\nunknown\n\nRequired\n\nmodel\n\nstring\n\nRequired\n\nID of the model to use.\n\ndimensions\n\ninteger\n\nThe number of dimensions the resulting output embeddings should have.\n\nencoding_format\n\nstring\n\nThe format to return the embeddings in. Can be either `float` or `base64`.\n\nuser\n\nstring\n\nA unique identifier representing your end-user, which can help xAI to monitor and detect abuse.\n\nHide optional fields\nDefinition\nExample\n\nPOST\n\n/v1/embeddings\n\n{\n  \"dimensions\": 0,\n  \"encoding_format\": \"\",\n  \"input\": null,\n  \"model\": \"\",\n  \"user\": \"\"\n}\n\n200\n\nResponse\n\n{\n  \"data\": [],\n  \"model\": \"\",\n  \"object\": \"\",\n  \"usage\": null\n}\n200\n400","propertyName2":"Definition\nExample\n\nGET\n\n/v1/embedding-models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400","propertyName3":null,"propertyName4":null,"propertyName5":null,"propertyName6":null},{"propertyName1":"#List embedding models\n\n/v1/embedding-models\n\nList all embedding models available for an API key.\n\nDefinition\nExample\n\nGET\n\n/v1/embedding-models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400","propertyName2":"Definition\nExample\n\nGET\n\n/v1/embedding-models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"created\": 0,\n  \"id\": \"\",\n  \"input_modalities\": [],\n  \"object\": \"\",\n  \"owned_by\": \"\",\n  \"prompt_image_token_price\": 0,\n  \"prompt_text_token_price\": 0,\n  \"version\": \"\"\n}\n200\n400\n404","propertyName3":null,"propertyName4":null,"propertyName5":null,"propertyName6":null},{"propertyName1":"#Get embedding model\n\n/v1/embedding-models/{model_id}\n\nEmbedding model retrieval with full information.\n\nPath parameters\n\nmodel_id\n\nstring\n\nID of the model to get.\n\nDefinition\nExample\n\nGET\n\n/v1/embedding-models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"created\": 0,\n  \"id\": \"\",\n  \"input_modalities\": [],\n  \"object\": \"\",\n  \"owned_by\": \"\",\n  \"prompt_image_token_price\": 0,\n  \"prompt_text_token_price\": 0,\n  \"version\": \"\"\n}\n200\n400\n404","propertyName2":"Definition\nExample\n\nGET\n\n/v1/language-models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400","propertyName3":null,"propertyName4":null,"propertyName5":null,"propertyName6":null},{"propertyName1":"#List language models\n\n/v1/language-models\n\nList all language models available.\n\nDefinition\nExample\n\nGET\n\n/v1/language-models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400","propertyName2":"Definition\nExample\n\nGET\n\n/v1/language-models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"completion_text_token_price\": 0,\n  \"created\": 0,\n  \"id\": \"\",\n  \"input_modalities\": [],\n  \"object\": \"\",\n  \"output_modalities\": [],\n  \"owned_by\": \"\",\n  \"prompt_image_token_price\": 0,\n  \"prompt_text_token_price\": 0,\n  \"version\": \"\"\n}\n200\n400\n404","propertyName3":null,"propertyName4":null,"propertyName5":null,"propertyName6":null},{"propertyName1":"#Get language model\n\n/v1/language-models/{model_id}\n\nGet information about an embedding model using its ID.\n\nPath parameters\n\nmodel_id\n\nstring\n\nID of the model to get.\n\nDefinition\nExample\n\nGET\n\n/v1/language-models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"completion_text_token_price\": 0,\n  \"created\": 0,\n  \"id\": \"\",\n  \"input_modalities\": [],\n  \"object\": \"\",\n  \"output_modalities\": [],\n  \"owned_by\": \"\",\n  \"prompt_image_token_price\": 0,\n  \"prompt_text_token_price\": 0,\n  \"version\": \"\"\n}\n200\n400\n404","propertyName2":"Definition\nExample\n\nGET\n\n/v1/models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400","propertyName3":null,"propertyName4":null,"propertyName5":null,"propertyName6":null},{"propertyName1":"#List models\n\n/v1/models\n\nOpenAI compatible version of model listing with reduced information. This endpoint is compatible with the OpenAI API.\n\nDefinition\nExample\n\nGET\n\n/v1/models\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"models\": []\n}\n200\n400","propertyName2":"Definition\nExample\n\nGET\n\n/v1/models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"created\": 0,\n  \"id\": \"\",\n  \"object\": \"\",\n  \"owned_by\": \"\"\n}\n200\n400\n404","propertyName3":null,"propertyName4":null,"propertyName5":null,"propertyName6":null},{"propertyName1":"#Get model\n\n/v1/models/{model_id}\n\nList all language and embedding models available. This endpoint is compatible with the OpenAI API.\n\nPath parameters\n\nmodel_id\n\nstring\n\nID of the model to get.\n\nDefinition\nExample\n\nGET\n\n/v1/models/{model_id}\n\nNo parameters.\n\n200\n\nResponse\n\n{\n  \"created\": 0,\n  \"id\": \"\",\n  \"object\": \"\",\n  \"owned_by\": \"\"\n}\n200\n400\n404","propertyName2":null,"propertyName3":null,"propertyName4":null,"propertyName5":null,"propertyName6":null}]